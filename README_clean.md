# üé¨ tol_vido ‚Äì YouTube to Clean Text (LLM Powered)

[![Build](https://github.com/TamerOnLine/pro_venv/actions/workflows/test-pro_venv.yml/badge.svg)](https://github.com/TamerOnLine/pro_venv/actions)
[![License](https://img.shields.io/github/license/TamerOnLine/pro_venv?style=flat-square)](LICENSE)

`tol_vido` is a powerful, local-first Python tool that transcribes YouTube videos into well-formatted German paragraphs using Large Language Models (LLMs) like **Mistral** via `.gguf`. It's fast, customizable, and runs entirely on your machine.

---

## üöÄ Features

- ‚úÖ Download and convert YouTube audio with `yt-dlp`
- ‚úÖ Segment and transcribe speech with `SpeechRecognition` (Google)
- ‚úÖ Format the raw text using a local LLM (e.g. Mistral via `llama-cpp-python`)
- ‚úÖ Full local processing ‚Äî **no cloud required**
- ‚úÖ Modular and extensible codebase
- ‚úÖ Auto-setup: project config, `venv`, VS Code files, and more

---

## üß± Project Structure

```bash
tol_vido/
‚îú‚îÄ‚îÄ 01_pro_venv.py          # Setup: venv, config, VS Code
‚îú‚îÄ‚îÄ 02_start_llm_server.py  # Run local LLM server
‚îú‚îÄ‚îÄ 03_main.py              # Entry point runner
‚îú‚îÄ‚îÄ app.py                  # Main processing logic
‚îú‚îÄ‚îÄ extract_text_from_video.py  # Transcribe audio from video
‚îú‚îÄ‚îÄ format_text.py          # Format using local LLM
‚îú‚îÄ‚îÄ output.txt              # Raw transcription result
‚îú‚îÄ‚îÄ formatted_output.txt    # Final cleaned-up version
‚îú‚îÄ‚îÄ setup-config.json       # Project metadata
‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
‚îú‚îÄ‚îÄ env-info.txt            # Python & installed packages
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ download_model.py   # GGUF model downloader
‚îî‚îÄ‚îÄ .github/workflows/      # GitHub Actions
```

---

## ‚ñ∂Ô∏è Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/YourUsername/tol_vido.git
cd tol_vido
```

### 2. Run the setup script

```bash
python 01_pro_venv.py
```

This will:
- Create a virtual environment
- Install dependencies
- Generate `main.py`, `app.py`, `.vscode/` settings

### 3. Start the LLM server
### ‚ö†Ô∏è Important: Pre-Run Setup

Before running the main script, make sure the following steps are completed:

#### üìÅ 1. Download and place FFmpeg binaries
This project requires FFmpeg tools for audio conversion.

- Download from: [https://www.gyan.dev/ffmpeg/builds/](https://www.gyan.dev/ffmpeg/builds/)
- Extract and copy the following files into a folder named `bin/` in the project root:

```bash
bin/
‚îú‚îÄ‚îÄ ffmpeg.exe
‚îú‚îÄ‚îÄ ffprobe.exe
‚îú‚îÄ‚îÄ ffplay.exe      # (optional)
```

> You **do not need** to add FFmpeg to your system PATH. The project uses them directly from `./bin/`.

---

#### üìÑ 2. Create `.env` file

In the project root, create a file named `.env` with the following content:

```env
# Required for downloading and running the model
MODEL_NAME=mistral-7b-instruct-v0.1.Q4_K_M.gguf

# Local path to the GGUF model file (adjust this path to match your setup)
MODEL=r:/tol_vido/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf

# LLM Server runtime parameters
N_CTX=4096
N_THREADS=8
N_GPU_LAYERS=35
VERBOSE=true
```

These values are used by:
- `tools/download_model.py` ‚Äì to fetch the model from Hugging Face
- `02_start_llm_server.py` ‚Äì to launch the local LLM inference server


```bash
python 02_start_llm_server.py
```

> Make sure the `.gguf` model exists in the `models/` folder. You can use `tools/download_model.py` to fetch it from Hugging Face.

### 4. Run the full pipeline

```bash
python 03_main.py
```

You‚Äôll be prompted for a YouTube URL. After processing, the result will be saved in:

- `output.txt`: raw transcription  
- `formatted_output.txt`: structured, punctuated paragraphs

---

## üì¶ Requirements
---

## üåê Model Setup (GGUF)
### 2. Download the model

```bash
python tools/download_model.py
```

Model will be downloaded from:
> [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)

---

## üí° Notes

- All audio processing and formatting are local.
- Transcription uses `Google Speech Recognition` ‚Äî no API key required.
- Formatting uses a **local** LLM server (default: `localhost:11434`).
- Works offline once the model is downloaded.

---

## üõ† VS Code Support

The setup script automatically generates:

- `.vscode/settings.json` ‚Üí sets interpreter path
- `.vscode/launch.json` ‚Üí debugger config
- `project.code-workspace` ‚Üí pre-configured workspace

---

## üìÑ License

This project is licensed under the [MIT License](LICENSE).

---

## üôå Acknowledgements

- [Mistral 7B](https://mistral.ai/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [yt-dlp](https://github.com/yt-dlp/yt-dlp)
- [tiktoken](https://github.com/openai/tiktoken)

---

> Made with ‚ù§Ô∏è by [Tamer Faour](https://github.com/TamerOnLine)